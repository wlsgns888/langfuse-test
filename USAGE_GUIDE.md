# Langfuse ìƒì„¸ ì‚¬ìš© ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” ê° ê¸°ëŠ¥ë³„ë¡œ ìƒì„¸í•œ ì‚¬ìš© ë°©ë²•ê³¼ ì‹¤ì „ í™œìš© íŒì„ ì œê³µí•©ë‹ˆë‹¤.

## ëª©ì°¨

1. [ê¸°ë³¸ íŠ¸ë ˆì´ì‹± (Tracing)](#1-ê¸°ë³¸-íŠ¸ë ˆì´ì‹±-tracing)
2. [Generations](#2-generations)
3. [Sessions](#3-sessions)
4. [Scoring](#4-scoring)
5. [Prompts](#5-prompts)
6. [Datasets](#6-datasets)
7. [Langchain í†µí•©](#7-langchain-í†µí•©)
8. [Agent êµ¬í˜„](#8-agent-êµ¬í˜„)

---

## 1. ê¸°ë³¸ íŠ¸ë ˆì´ì‹± (Tracing)

### ê°œë… ì´í•´

**Trace**ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì „ì²´ ì‹¤í–‰ íë¦„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
**Span**ì€ Trace ë‚´ì˜ ê°œë³„ ì‘ì—… ë‹¨ìœ„ì…ë‹ˆë‹¤.

```
Trace (ì „ì²´ ìš”ì²­)
â”œâ”€â”€ Span 1 (ì „ì²˜ë¦¬)
â”œâ”€â”€ Span 2 (LLM í˜¸ì¶œ)
â”‚   â”œâ”€â”€ Span 2.1 (í”„ë¡¬í”„íŠ¸ êµ¬ì„±)
â”‚   â””â”€â”€ Span 2.2 (API í˜¸ì¶œ)
â””â”€â”€ Span 3 (í›„ì²˜ë¦¬)
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from langfuse import Langfuse

langfuse = Langfuse()

# Trace ìƒì„±
trace = langfuse.trace(
    name="user_request",
    user_id="user_123",
    metadata={"version": "1.0"},
    tags=["production", "api"]
)

# Span ì¶”ê°€
span = trace.span(
    name="data_processing",
    input={"data": "..."},
    metadata={"step": 1}
)

# ì‘ì—… ìˆ˜í–‰
result = process_data()

# Span ì¢…ë£Œ
span.end(output=result)

# Trace ì¢…ë£Œ
trace.end()

# ë°ì´í„° ì „ì†¡
langfuse.flush()
```

### ì‹¤ì „ í™œìš© íŒ

1. **ê³„ì¸µì  êµ¬ì¡° í™œìš©**: ë³µì¡í•œ ì‘ì—…ì€ ì¤‘ì²© Spanìœ¼ë¡œ í‘œí˜„
2. **ë©”íƒ€ë°ì´í„° í™œìš©**: ë””ë²„ê¹…ì— í•„ìš”í•œ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
3. **íƒœê·¸ í™œìš©**: í™˜ê²½(dev/prod), ê¸°ëŠ¥, íŒ€ë³„ë¡œ íƒœê·¸ ì§€ì •
4. **ì—ëŸ¬ ì¶”ì **: level="ERROR"ë¡œ ì—ëŸ¬ ìƒíƒœ ê¸°ë¡

### ì‹¤í–‰ ëª…ë ¹

```bash
python 01_basic_tracing.py
```

---

## 2. Generations

### ê°œë… ì´í•´

**Generation**ì€ LLMì˜ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì¶”ì í•˜ëŠ” íŠ¹ë³„í•œ Observationì…ë‹ˆë‹¤.
ìë™ìœ¼ë¡œ í† í° ìˆ˜, ë¹„ìš©, ë ˆì´í„´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
trace = langfuse.trace(name="llm_call")

generation = trace.generation(
    name="gpt4_response",
    model="gpt-4",
    model_parameters={
        "temperature": 0.7,
        "max_tokens": 500
    },
    input=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)

# LLM í˜¸ì¶œ í›„
generation.end(
    output="Hi! How can I help you?",
    usage={
        "prompt_tokens": 20,
        "completion_tokens": 8,
        "total_tokens": 28
    }
)
```

### í™œìš© ì‹œë‚˜ë¦¬ì˜¤

1. **ëŒ€í™”í˜• ì• í”Œë¦¬ì¼€ì´ì…˜**: ê° í„´ì„ ë³„ë„ Generationìœ¼ë¡œ ì¶”ì 
2. **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: ì²­í¬ ë‹¨ìœ„ ì¶”ì 
3. **ëª¨ë¸ ë¹„êµ**: ë™ì¼ ì…ë ¥ìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸
4. **ë¹„ìš© ë¶„ì„**: í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ê³„ì‚°

### ë¹„ìš© ê³„ì‚° ì˜ˆì œ

```python
MODEL_COSTS = {
    "gpt-4": {
        "prompt": 0.03 / 1000,
        "completion": 0.06 / 1000
    }
}

cost = (
    prompt_tokens * MODEL_COSTS["gpt-4"]["prompt"] +
    completion_tokens * MODEL_COSTS["gpt-4"]["completion"]
)
```

### ì‹¤í–‰ ëª…ë ¹

```bash
python 02_generations.py
```

---

## 3. Sessions

### ê°œë… ì´í•´

**Session**ì€ ê´€ë ¨ëœ ì—¬ëŸ¬ Traceë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì™€ì˜ ì „ì²´ ëŒ€í™”
- íŠ¹ì • ì‘ì—…ì˜ ì—¬ëŸ¬ ì‹œë„
- ì‚¬ìš©ì ì—¬ì •

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
session_id = "session_001"

# ì²« ë²ˆì§¸ ìƒí˜¸ì‘ìš©
trace1 = langfuse.trace(
    name="interaction_1",
    session_id=session_id,
    user_id="user_123"
)
# ... ì‘ì—… ìˆ˜í–‰

# ë‘ ë²ˆì§¸ ìƒí˜¸ì‘ìš©
trace2 = langfuse.trace(
    name="interaction_2",
    session_id=session_id,
    user_id="user_123"
)
# ... ì‘ì—… ìˆ˜í–‰
```

### í™œìš© ì‹œë‚˜ë¦¬ì˜¤

1. **ê³ ê° ì§€ì›**: ì „ì²´ ì§€ì› ì„¸ì…˜ ì¶”ì 
2. **ì±—ë´‡**: ëŒ€í™” ì„¸ì…˜ë³„ ê·¸ë£¹í™”
3. **ì‚¬ìš©ì ì—¬ì •**: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‚¬ìš©ì í–‰ë™ ì¶”ì 
4. **ë””ë²„ê¹…**: ë¬¸ì œê°€ ë°œìƒí•œ ì„¸ì…˜ ì „ì²´ ë¶„ì„

### ì„¸ì…˜ ë©”íƒ€ë°ì´í„° ì˜ˆì œ

```python
session_metadata = {
    "user_tier": "premium",
    "device": "mobile",
    "platform": "iOS",
    "app_version": "2.5.1",
    "session_start": datetime.now().isoformat()
}

trace = langfuse.trace(
    session_id=session_id,
    metadata=session_metadata
)
```

### ì‹¤í–‰ ëª…ë ¹

```bash
python 03_sessions.py
```

---

## 4. Scoring

### ê°œë… ì´í•´

**Scoring**ì€ LLM ì¶œë ¥ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- ìë™ ë©”íŠ¸ë¦­ (ì •í™•ë„, ê´€ë ¨ì„±)
- ì‚¬ìš©ì í”¼ë“œë°± (ì¢‹ì•„ìš”/ì‹«ì–´ìš”, ë³„ì )
- ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
trace = langfuse.trace(name="evaluated_response")

# Generation ì‹¤í–‰
generation = trace.generation(...)
generation.end(output=response)

# ì ìˆ˜ ì¶”ê°€
trace.score(
    name="accuracy",
    value=0.95,
    comment="Very accurate response"
)

trace.score(
    name="user_feedback",
    value=1.0,  # 1.0 = ì¢‹ì•„ìš”, 0.0 = ì‹«ì–´ìš”
    data_type="BOOLEAN"
)
```

### ì ìˆ˜ ìœ í˜•

1. **NUMERIC**: 0-1 ì‚¬ì´ì˜ ì—°ì† ê°’
2. **BOOLEAN**: True/False (1.0/0.0)
3. **CATEGORICAL**: ì¹´í…Œê³ ë¦¬ ë ˆì´ë¸”

### í™œìš© ì‹œë‚˜ë¦¬ì˜¤

1. **í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì¶œë ¥ í’ˆì§ˆ ì¶”ì 
2. **ì‚¬ìš©ì í”¼ë“œë°±**: ì¢‹ì•„ìš”/ì‹«ì–´ìš”, ë³„ì  ìˆ˜ì§‘
3. **A/B í…ŒìŠ¤íŠ¸**: ëª¨ë¸ ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ë²„ì „ ë¹„êµ
4. **ì•Œë¦¼ ì‹œìŠ¤í…œ**: ì ìˆ˜ê°€ ì„ê³„ê°’ ë¯¸ë§Œì¼ ë•Œ ì•Œë¦¼

### A/B í…ŒìŠ¤íŠ¸ ì˜ˆì œ

```python
# Version A
trace_a = langfuse.trace(
    name="test_version_a",
    metadata={"variant": "A"}
)
# ... ì‹¤í–‰
trace_a.score(name="quality", value=0.85)

# Version B
trace_b = langfuse.trace(
    name="test_version_b",
    metadata={"variant": "B"}
)
# ... ì‹¤í–‰
trace_b.score(name="quality", value=0.92)
```

### ì‹¤í–‰ ëª…ë ¹

```bash
python 04_scoring.py
```

---

## 5. Prompts

### ê°œë… ì´í•´

**Prompt Management**ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì½”ë“œì—ì„œ ë¶„ë¦¬í•˜ì—¬ ì¤‘ì•™ì—ì„œ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
- ë²„ì „ ê´€ë¦¬
- A/B í…ŒìŠ¤íŠ¸
- í”„ë¡œë•ì…˜/ê°œë°œ ë¶„ë¦¬

### Langfuse UIì—ì„œ í”„ë¡¬í”„íŠ¸ ìƒì„±

1. Langfuse ëŒ€ì‹œë³´ë“œ ì ‘ì†
2. "Prompts" ë©”ë‰´ í´ë¦­
3. "Create New Prompt" í´ë¦­
4. ì´ë¦„, í…œí”Œë¦¿, ë³€ìˆ˜ ì„¤ì •
5. ì €ì¥ í›„ ë²„ì „ íƒœê¹…

### ì½”ë“œì—ì„œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©

```python
from langfuse import Langfuse

langfuse = Langfuse()

# í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
prompt = langfuse.get_prompt(
    name="qa_assistant",
    version="production"  # ë˜ëŠ” íŠ¹ì • ë²„ì „ ë²ˆí˜¸
)

# ë³€ìˆ˜ ì ìš©
final_prompt = prompt.compile(
    question="What is Langfuse?"
)

# Generationì— í”„ë¡¬í”„íŠ¸ ì •ë³´ í¬í•¨
generation = trace.generation(
    name="response",
    model="gpt-3.5-turbo",
    input=final_prompt,
    prompt={
        "name": "qa_assistant",
        "version": "production"
    }
)
```

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì œ

```
You are a {{assistant_type}}.
{{additional_instructions}}

User question: {{question}}
```

### í™œìš© ì‹œë‚˜ë¦¬ì˜¤

1. **ë²„ì „ ê´€ë¦¬**: í”„ë¡¬í”„íŠ¸ ë³€ê²½ ì´ë ¥ ì¶”ì 
2. **A/B í…ŒìŠ¤íŠ¸**: ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë²„ì „ ë™ì‹œ í…ŒìŠ¤íŠ¸
3. **ë¡¤ë°±**: ë¬¸ì œ ë°œìƒ ì‹œ ì´ì „ ë²„ì „ìœ¼ë¡œ ì¦‰ì‹œ ë³µêµ¬
4. **í˜‘ì—…**: íŒ€ì› ê°„ í”„ë¡¬í”„íŠ¸ ê³µìœ  ë° ê°œì„ 

### ì‹¤í–‰ ëª…ë ¹

```bash
python 05_prompts.py
```

---

## 6. Datasets

### ê°œë… ì´í•´

**Datasets**ëŠ” ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ëª¨ìŒì…ë‹ˆë‹¤.
- ì¼ê´€ëœ í‰ê°€
- íšŒê·€ í…ŒìŠ¤íŠ¸
- ë²¤ì¹˜ë§ˆí‚¹

### ë°ì´í„°ì…‹ ìƒì„±

```python
# Langfuse UI ë˜ëŠ” APIë¥¼ í†µí•´ ìƒì„±
dataset = langfuse.create_dataset(
    name="qa_eval_dataset",
    description="QA system evaluation dataset"
)

# ì•„ì´í…œ ì¶”ê°€
dataset.create_item(
    input={"question": "What is the capital of France?"},
    expected_output="Paris",
    metadata={"category": "geography"}
)
```

### ë°ì´í„°ì…‹ í‰ê°€ ì‹¤í–‰

```python
# ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
dataset = langfuse.get_dataset("qa_eval_dataset")

# ê° ì•„ì´í…œ í‰ê°€
for item in dataset.items:
    trace = langfuse.trace(
        name=f"eval_{item.id}",
        metadata={"dataset_run": "run_001"}
    )

    # ëª¨ë¸ ì‹¤í–‰
    output = model.generate(item.input)

    # ì •í™•ë„ ê³„ì‚°
    is_correct = output == item.expected_output

    # ì ìˆ˜ ê¸°ë¡
    trace.score(
        name="correctness",
        value=1.0 if is_correct else 0.0
    )
```

### í™œìš© ì‹œë‚˜ë¦¬ì˜¤

1. **ëª¨ë¸ í‰ê°€**: ìƒˆ ëª¨ë¸ ë˜ëŠ” ë²„ì „ í‰ê°€
2. **íšŒê·€ í…ŒìŠ¤íŠ¸**: ì—…ë°ì´íŠ¸ í›„ ì„±ëŠ¥ í™•ì¸
3. **ë²¤ì¹˜ë§ˆí‚¹**: ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
4. **í’ˆì§ˆ ê²Œì´íŠ¸**: ë°°í¬ ì „ ìµœì†Œ ê¸°ì¤€ í™•ì¸

### ì‹¤í–‰ ëª…ë ¹

```bash
python 06_datasets.py
```

---

## 7. Langchain í†µí•©

### ê°œë… ì´í•´

**CallbackHandler**ë¥¼ ì‚¬ìš©í•˜ì—¬ Langchainì˜ ëª¨ë“  ì‘ì—…ì„ ìë™ìœ¼ë¡œ Langfuseì— ì¶”ì í•©ë‹ˆë‹¤.

### ê¸°ë³¸ ì„¤ì •

```python
from langfuse.callback import CallbackHandler

# ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
handler = CallbackHandler()

# Trace ë©”íƒ€ë°ì´í„° ì„¤ì •
handler.trace(
    name="my_chain",
    user_id="user_123",
    metadata={"framework": "langchain"}
)
```

### Langchainê³¼ í•¨ê»˜ ì‚¬ìš©

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")

# ì½œë°± í•¸ë“¤ëŸ¬ ì „ë‹¬
response = llm.invoke(
    "Hello!",
    config={"callbacks": [handler]}
)
```

### Chain ì¶”ì 

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    template="Translate {text} to {language}",
    input_variables=["text", "language"]
)

chain = LLMChain(llm=llm, prompt=prompt)

result = chain.invoke(
    {"text": "Hello", "language": "French"},
    config={"callbacks": [handler]}
)
```

### RAG íŒŒì´í”„ë¼ì¸ ì¶”ì 

```python
# 1. ë¬¸ì„œ ê²€ìƒ‰
retriever = vectorstore.as_retriever()
docs = retriever.get_relevant_documents(
    query,
    callbacks=[handler]
)

# 2. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
context = "\n".join([doc.page_content for doc in docs])

# 3. LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
response = llm.invoke(
    f"Answer based on context: {context}\nQuestion: {query}",
    config={"callbacks": [handler]}
)
```

### ìë™ ì¶”ì ë˜ëŠ” ì •ë³´

- LLM í˜¸ì¶œ (ì…ë ¥, ì¶œë ¥, í† í°)
- Chain ì‹¤í–‰ (ê° ë‹¨ê³„)
- Tool ì‚¬ìš©
- ë¬¸ì„œ ê²€ìƒ‰
- ì‹¤í–‰ ì‹œê°„
- ì—ëŸ¬

### ì‹¤í–‰ ëª…ë ¹

```bash
python 07_langchain_integration.py
```

---

## 8. Agent êµ¬í˜„

### ê°œë… ì´í•´

**Agent**ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ììœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

**ReAct íŒ¨í„´**:
1. **Thought**: ë¬´ì—‡ì„ í•´ì•¼ í•˜ëŠ”ì§€ ìƒê°
2. **Action**: ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰
3. **Observation**: ê²°ê³¼ í™•ì¸
4. **Repeat**: í•„ìš”ì‹œ ë°˜ë³µ
5. **Answer**: ìµœì¢… ë‹µë³€

### ì»¤ìŠ¤í…€ ë„êµ¬ ì •ì˜

```python
from langchain_core.tools import tool

@tool
def calculator(expression: str) -> str:
    """ìˆ˜í•™ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        result = eval(expression)
        return f"ê³„ì‚° ê²°ê³¼: {result}"
    except Exception as e:
        return f"ê³„ì‚° ì˜¤ë¥˜: {str(e)}"

@tool
def search(query: str) -> str:
    """ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ê²€ìƒ‰ ë¡œì§
    return search_results
```

### Agent ì‹¤í–‰ ì˜ˆì œ

```python
from langfuse import Langfuse

langfuse = Langfuse()

# Trace ìƒì„±
trace = langfuse.trace(
    name="agent_execution",
    metadata={"agent_type": "react"}
)

# Step 1: Thought
thought = "ì‚¬ìš©ìê°€ ê³„ì‚°ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤."
thought_span = trace.span(name="thought_1")
thought_span.end(output=thought)

# Step 2: Action
action_span = trace.span(name="action_1")
result = calculator.func("10 + 5")
action_span.end(
    input="10 + 5",
    output=result
)

# Step 3: Final Answer
answer_span = trace.span(name="final_answer")
answer = f"The result is {result}"
answer_span.end(output=answer)

trace.end()
```

### ë‹¤ë‹¨ê³„ Agent

```python
# ë³µì¡í•œ ì¿¼ë¦¬: "ë‚ ì”¨ë¥¼ í™•ì¸í•˜ê³  ë©”ëª¨ë¡œ ì €ì¥"

# Iteration 1: ë‚ ì”¨ í™•ì¸
trace.span(name="step_1_weather")
weather = get_weather("Seoul")

# Iteration 2: ë©”ëª¨ ì €ì¥
trace.span(name="step_2_save")
save_note(f"Weather: {weather}")

# Final Answer
answer = "I checked the weather and saved it to your notes."
```

### ì—ëŸ¬ ì²˜ë¦¬

```python
try:
    result = tool.func(input)
    span.end(output=result)
except Exception as e:
    span.end(
        level="ERROR",
        status_message=str(e)
    )
    # ë³µêµ¬ ë¡œì§
    recovery_span = trace.span(name="error_recovery")
    # ...
```

### Agent ì„±ëŠ¥ ìµœì í™”

1. **ë„êµ¬ ì„ íƒ ìµœì í™”**: í•„ìš”í•œ ë„êµ¬ë§Œ ì œê³µ
2. **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**: ëŒ€í™” íˆìŠ¤í† ë¦¬ ì ì ˆíˆ ê´€ë¦¬
3. **ì¡°ê¸° ì¢…ë£Œ**: ë‹µë³€ì„ ì°¾ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ
4. **ë³‘ë ¬ ì‹¤í–‰**: ë…ë¦½ì ì¸ ë„êµ¬ëŠ” ë³‘ë ¬ë¡œ ì‹¤í–‰

### ì‹¤í–‰ ëª…ë ¹

```bash
python 08_agent_with_langfuse.py
```

---

## ëŒ€ì‹œë³´ë“œ í™œìš©

### Traces í˜ì´ì§€

- ëª¨ë“  ì‹¤í–‰ ê¸°ë¡ ì¡°íšŒ
- í•„í„°ë§ ë° ê²€ìƒ‰
- ìƒì„¸ ì •ë³´ ë“œë¦´ë‹¤ìš´

### Sessions í˜ì´ì§€

- ì„¸ì…˜ë³„ ê·¸ë£¹í™”
- ì‚¬ìš©ìë³„ ë¶„ì„
- ì‹œê°„ë³„ íŒ¨í„´

### Scores í˜ì´ì§€

- ì ìˆ˜ ë¶„í¬
- ì‹œê°„ì— ë”°ë¥¸ ì¶”ì´
- ëª¨ë¸ë³„ ë¹„êµ

### Analytics í˜ì´ì§€

- ë¹„ìš© ë¶„ì„
- ë ˆì´í„´ì‹œ ëª¨ë‹ˆí„°ë§
- í† í° ì‚¬ìš©ëŸ‰

---

## ì‹¤ì „ í™œìš© íŒ

### 1. ê°œë°œ vs í”„ë¡œë•ì…˜

```python
import os

environment = os.getenv("ENVIRONMENT", "development")

trace = langfuse.trace(
    name="my_app",
    tags=[environment],
    metadata={"env": environment}
)
```

### 2. ë¹„ìš© ëª¨ë‹ˆí„°ë§

```python
# ì¼ì¼ ë¹„ìš© ì¶”ì 
daily_cost = sum([
    generation.usage.total_cost
    for generation in langfuse.get_generations(
        from_timestamp=today_start
    )
])

if daily_cost > DAILY_BUDGET:
    send_alert("Daily budget exceeded")
```

### 3. í’ˆì§ˆ ì•Œë¦¼

```python
# í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ì„ ë•Œ ì•Œë¦¼
if score < QUALITY_THRESHOLD:
    trace.score(
        name="quality_alert",
        value=score,
        comment="Quality below threshold"
    )
    notify_team()
```

### 4. A/B í…ŒìŠ¤íŠ¸ ìë™í™”

```python
import random

variant = "A" if random.random() < 0.5 else "B"

trace = langfuse.trace(
    metadata={"variant": variant}
)

if variant == "A":
    prompt = prompt_version_a
else:
    prompt = prompt_version_b
```

---

## ë¬¸ì œ í•´ê²°

### ë°ì´í„°ê°€ ëŒ€ì‹œë³´ë“œì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠìŒ

```python
# flush() í˜¸ì¶œ í™•ì¸
langfuse.flush()

# ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
with langfuse:
    # ì‘ì—… ìˆ˜í–‰
    pass  # ìë™ìœ¼ë¡œ flushë¨
```

### ì„±ëŠ¥ ìµœì í™”

```python
# ë°°ì¹˜ ì²˜ë¦¬ ì‚¬ìš©
langfuse.batch_size = 50  # ê¸°ë³¸ê°’: 15

# ë¹„ë™ê¸° ì „ì†¡
langfuse.enabled = True
langfuse.background_flush = True
```

---

## ë‹¤ìŒ ë‹¨ê³„

1. ì˜ˆì œ íŒŒì¼ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”
2. ìì‹ ì˜ use caseì— ë§ê²Œ ì½”ë“œë¥¼ ìˆ˜ì •í•´ë³´ì„¸ìš”
3. ëŒ€ì‹œë³´ë“œì—ì„œ ë°ì´í„°ë¥¼ íƒìƒ‰í•˜ì„¸ìš”
4. ì‹¤ì œ í”„ë¡œì íŠ¸ì— í†µí•©í•´ë³´ì„¸ìš”

**Happy Learning! ğŸ“**
